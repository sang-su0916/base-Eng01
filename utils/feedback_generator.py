import spacy
from textblob import TextBlob
import re

class FeedbackGenerator:
    def __init__(self):
        """ì²¨ì‚­ ìƒì„±ê¸° ì´ˆê¸°í™”"""
        # ì˜ì–´ ì–¸ì–´ ëª¨ë¸ ë¡œë“œ
        self.nlp = spacy.load("en_core_web_sm")
        
    def generate_detailed_feedback(self, student_answer, model_answer, problem_type):
        """í•™ìƒ ë‹µì•ˆì— ëŒ€í•œ ìƒì„¸ í”¼ë“œë°± ìƒì„±"""
        feedback = {
            "overall_score": 0,
            "grammar_feedback": [],
            "content_feedback": [],
            "vocabulary_feedback": [],
            "suggestions": [],
            "positive_points": [],
            "korean_summary": ""
        }
        
        # ë¬¸ì œ ìœ í˜•ë³„ ë¶„ì„
        if problem_type == "ì˜ì‘ë¬¸":
            feedback = self._analyze_writing(student_answer, model_answer, feedback)
        elif problem_type == "ë¬¸ë²•":
            feedback = self._analyze_grammar(student_answer, model_answer, feedback)
        elif problem_type == "ì–´íœ˜":
            feedback = self._analyze_vocabulary(student_answer, model_answer, feedback)
        else:
            feedback = self._analyze_general(student_answer, model_answer, feedback)
            
        # ì¢…í•© ì ìˆ˜ ê³„ì‚°
        feedback["overall_score"] = self._calculate_overall_score(feedback)
        
        # í•œê¸€ ìš”ì•½ ìƒì„±
        feedback["korean_summary"] = self._generate_korean_summary(feedback)
        
        return feedback

    def _check_basic_grammar(self, doc):
        """ê¸°ë³¸ ë¬¸ë²• ê·œì¹™ ê²€ì‚¬"""
        errors = []
        
        # ì£¼ì–´-ë™ì‚¬ ì¼ì¹˜ ê²€ì‚¬
        for sent in doc.sents:
            subject = None
            main_verb = None
            
            for token in sent:
                # ì£¼ì–´ ì°¾ê¸°
                if token.dep_ == "nsubj":
                    subject = token
                # ì£¼ë™ì‚¬ ì°¾ê¸°
                if token.pos_ == "VERB" and token.dep_ in ["ROOT", "VERB"]:
                    main_verb = token
                    
            if subject and main_verb:
                # 3ì¸ì¹­ ë‹¨ìˆ˜ í˜„ì¬í˜• ê²€ì‚¬
                if subject.text.lower() in ["he", "she", "it"] and \
                   main_verb.tag_ == "VBP":  # base form instead of 3rd person
                    errors.append({
                        "error": "ì£¼ì–´-ë™ì‚¬ ë¶ˆì¼ì¹˜",
                        "context": f"{subject.text} {main_verb.text}",
                        "suggestion": f"{subject.text} {main_verb.text}s",
                        "explanation": "3ì¸ì¹­ ë‹¨ìˆ˜ ì£¼ì–´ëŠ” ë™ì‚¬ì— -së¥¼ ë¶™ì—¬ì•¼ í•©ë‹ˆë‹¤."
                    })
        
        # ê´€ì‚¬ ì‚¬ìš© ê²€ì‚¬
        for token in doc:
            if token.pos_ == "NOUN" and token.dep_ not in ["compound"]:
                has_det = any(child.pos_ == "DET" for child in token.children)
                if not has_det and not token.tag_ == "NNS":  # ë³µìˆ˜í˜•ì´ ì•„ë‹Œ ê²½ìš°
                    errors.append({
                        "error": "ê´€ì‚¬ ëˆ„ë½",
                        "context": token.text,
                        "suggestion": f"a/the {token.text}",
                        "explanation": "ê°€ì‚°ëª…ì‚¬ ë‹¨ìˆ˜í˜• ì•ì—ëŠ” ê´€ì‚¬ê°€ í•„ìš”í•©ë‹ˆë‹¤."
                    })
        
        return errors

    def _analyze_writing(self, student_answer, model_answer, feedback):
        """ì˜ì‘ë¬¸ ë¶„ì„"""
        student_doc = self.nlp(student_answer)
        
        # ê¸°ë³¸ ë¬¸ë²• ì˜¤ë¥˜ ê²€ì‚¬
        grammar_errors = self._check_basic_grammar(student_doc)
        feedback["grammar_feedback"].extend(grammar_errors)
        
        # ë¬¸ì¥ êµ¬ì¡° ë¶„ì„
        feedback["content_feedback"].extend(self._analyze_sentence_structure(student_doc))
        
        # ì–´íœ˜ ë‹¤ì–‘ì„± ë¶„ì„
        feedback["vocabulary_feedback"].extend(self._analyze_vocabulary_diversity(student_doc))
        
        # ê¸ì •ì ì¸ ë¶€ë¶„ ì°¾ê¸°
        feedback["positive_points"].extend(self._find_positive_points(student_answer, model_answer))
        
        return feedback

    def _analyze_grammar(self, student_answer, model_answer, feedback):
        """ë¬¸ë²• ë¬¸ì œ ë¶„ì„"""
        student_doc = self.nlp(student_answer)
        
        # ê¸°ë³¸ ë¬¸ë²• ì˜¤ë¥˜ ê²€ì‚¬
        grammar_errors = self._check_basic_grammar(student_doc)
        feedback["grammar_feedback"].extend(grammar_errors)
        
        # íŠ¹ì • ë¬¸ë²• ìš”ì†Œ ë¶„ì„
        feedback["grammar_feedback"].extend(self._analyze_specific_grammar(student_doc))
        
        return feedback

    def _analyze_vocabulary(self, student_answer, model_answer, feedback):
        """ì–´íœ˜ ë¬¸ì œ ë¶„ì„"""
        student_words = set(word.lower() for word in student_answer.split())
        model_words = set(word.lower() for word in model_answer.split())
        
        # ëˆ„ë½ëœ ì£¼ìš” ì–´íœ˜ í™•ì¸
        missing_words = model_words - student_words
        if missing_words:
            feedback["vocabulary_feedback"].append({
                "point": "ëˆ„ë½ëœ ì£¼ìš” ì–´íœ˜",
                "details": list(missing_words),
                "suggestion": "ë‹¤ìŒ ë‹¨ì–´ë“¤ì„ í¬í•¨í•˜ë©´ ì¢‹ì•˜ì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤."
            })
        
        # ë¶€ì ì ˆí•œ ë‹¨ì–´ ì‚¬ìš© í™•ì¸
        student_doc = self.nlp(student_answer)
        feedback["vocabulary_feedback"].extend(self._analyze_word_choice(student_doc))
        
        return feedback

    def _analyze_general(self, student_answer, model_answer, feedback):
        """ì¼ë°˜ì ì¸ ë¬¸ì œ ë¶„ì„"""
        # ê¸°ë³¸ì ì¸ ë¹„êµ ë¶„ì„
        student_blob = TextBlob(student_answer)
        model_blob = TextBlob(model_answer)
        
        # ê°ì • ë¶„ì„
        if student_blob.sentiment.polarity != model_blob.sentiment.polarity:
            feedback["content_feedback"].append({
                "point": "ê¸€ì˜ ì–´ì¡°",
                "details": "ë‹µì•ˆì˜ ì „ë°˜ì ì¸ ì–´ì¡°ê°€ ëª¨ë²” ë‹µì•ˆê³¼ ë‹¤ë¦…ë‹ˆë‹¤.",
                "suggestion": "ê¸€ì˜ ëª©ì ì— ë§ëŠ” ì–´ì¡°ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”."
            })
        
        # ë¬¸ì¥ ê¸¸ì´ ë¶„ì„
        if len(student_blob.sentences) != len(model_blob.sentences):
            feedback["content_feedback"].append({
                "point": "ë¬¸ì¥ êµ¬ì„±",
                "details": f"ëª¨ë²” ë‹µì•ˆì€ {len(model_blob.sentences)}ê°œì˜ ë¬¸ì¥ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤.",
                "suggestion": "ì ì ˆí•œ ë¬¸ì¥ ë¶„í• ì„ ê³ ë ¤í•´ë³´ì„¸ìš”."
            })
        
        return feedback

    def _analyze_sentence_structure(self, doc):
        """ë¬¸ì¥ êµ¬ì¡° ë¶„ì„"""
        feedback = []
        
        # ë¬¸ì¥ ê¸¸ì´ ë¶„ì„
        sent_lengths = [len(sent) for sent in doc.sents]
        avg_length = sum(sent_lengths) / len(sent_lengths) if sent_lengths else 0
        
        if avg_length > 30:
            feedback.append({
                "point": "ë¬¸ì¥ ê¸¸ì´",
                "details": "ë¬¸ì¥ì´ ë„ˆë¬´ ê¸¸ì–´ ì´í•´í•˜ê¸° ì–´ë ¤ìš¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
                "suggestion": "ê¸´ ë¬¸ì¥ì„ ì—¬ëŸ¬ ê°œì˜ ì§§ì€ ë¬¸ì¥ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ë³´ì„¸ìš”."
            })
        
        # êµ¬ì¡° ë‹¤ì–‘ì„± ë¶„ì„
        sentence_starts = []
        for sent in doc.sents:
            first_token = next(sent.iter_tokens())
            sentence_starts.append(first_token.pos_)
        
        if len(set(sentence_starts)) < 2:
            feedback.append({
                "point": "ë¬¸ì¥ êµ¬ì¡° ë‹¤ì–‘ì„±",
                "details": "ë¹„ìŠ·í•œ êµ¬ì¡°ì˜ ë¬¸ì¥ì´ ë°˜ë³µë©ë‹ˆë‹¤.",
                "suggestion": "ë‹¤ì–‘í•œ ë¬¸ì¥ êµ¬ì¡°ë¥¼ ì‚¬ìš©í•˜ì—¬ ê¸€ì˜ íë¦„ì„ ê°œì„ í•´ë³´ì„¸ìš”."
            })
        
        return feedback

    def _analyze_vocabulary_diversity(self, doc):
        """ì–´íœ˜ ë‹¤ì–‘ì„± ë¶„ì„"""
        feedback = []
        
        # ì–´íœ˜ ë‹¤ì–‘ì„± ê³„ì‚°
        words = [token.text.lower() for token in doc if token.is_alpha]
        unique_words = set(words)
        
        if len(words) > 0:
            diversity_ratio = len(unique_words) / len(words)
            
            if diversity_ratio < 0.4:  # 40% ë¯¸ë§Œì˜ ì–´íœ˜ ë‹¤ì–‘ì„±
                feedback.append({
                    "point": "ì–´íœ˜ ë‹¤ì–‘ì„±",
                    "details": "ê°™ì€ ë‹¨ì–´ê°€ ìì£¼ ë°˜ë³µë˜ê³  ìˆìŠµë‹ˆë‹¤.",
                    "suggestion": "ìœ ì˜ì–´ë¥¼ í™œìš©í•˜ì—¬ ë” ë‹¤ì–‘í•œ í‘œí˜„ì„ ì‹œë„í•´ë³´ì„¸ìš”."
                })
        
        # ê³ ê¸‰ ì–´íœ˜ ì‚¬ìš© ë¶„ì„
        advanced_words = 0
        for token in doc:
            if token.is_alpha and len(token.text) > 8:  # 8ê¸€ì ì´ìƒì˜ ë‹¨ì–´ë¥¼ ê³ ê¸‰ ì–´íœ˜ë¡œ ê°„ì£¼
                advanced_words += 1
        
        if len(words) > 0 and advanced_words / len(words) < 0.1:  # ê³ ê¸‰ ì–´íœ˜ ë¹„ìœ¨ 10% ë¯¸ë§Œ
            feedback.append({
                "point": "ì–´íœ˜ ìˆ˜ì¤€",
                "details": "ê¸°ì´ˆì ì¸ ì–´íœ˜ê°€ ì£¼ë¡œ ì‚¬ìš©ë˜ì—ˆìŠµë‹ˆë‹¤.",
                "suggestion": "ìƒí™©ì— ë§ëŠ” ë” ì •í™•í•˜ê³  ì„¸ë ¨ëœ ì–´íœ˜ë¥¼ ì‚¬ìš©í•´ë³´ì„¸ìš”."
            })
        
        return feedback

    def _analyze_word_choice(self, doc):
        """ë‹¨ì–´ ì„ íƒ ë¶„ì„"""
        feedback = []
        
        # ìì£¼ ì‚¬ìš©ë˜ëŠ” ê¸°ì´ˆ ë™ì‚¬ ëª©ë¡
        basic_verbs = {"be", "have", "do", "make", "get", "go", "take", "come", "see", "know"}
        
        # ê¸°ì´ˆ ë™ì‚¬ ì‚¬ìš© ë¹ˆë„ ë¶„ì„
        basic_verb_count = 0
        total_verbs = 0
        
        for token in doc:
            if token.pos_ == "VERB":
                total_verbs += 1
                if token.lemma_.lower() in basic_verbs:
                    basic_verb_count += 1
        
        if total_verbs > 0 and basic_verb_count / total_verbs > 0.6:  # ê¸°ì´ˆ ë™ì‚¬ ë¹„ìœ¨ 60% ì´ˆê³¼
            feedback.append({
                "point": "ë™ì‚¬ ì„ íƒ",
                "details": "ê¸°ì´ˆì ì¸ ë™ì‚¬ê°€ ë§ì´ ì‚¬ìš©ë˜ì—ˆìŠµë‹ˆë‹¤.",
                "suggestion": "ë” êµ¬ì²´ì ì´ê³  ìƒí™©ì— ë§ëŠ” ë™ì‚¬ë¥¼ ì‚¬ìš©í•´ë³´ì„¸ìš”."
            })
        
        # ë¶€ì ì ˆí•œ ë‹¨ì–´ ì¡°í•© ë¶„ì„
        for i in range(len(doc) - 1):
            token = doc[i]
            next_token = doc[i + 1]
            
            if token.pos_ == "ADJ" and next_token.pos_ == "ADJ":
                feedback.append({
                    "point": "í˜•ìš©ì‚¬ ì¤‘ë³µ",
                    "details": f"'{token.text} {next_token.text}'ì™€ ê°™ì´ í˜•ìš©ì‚¬ê°€ ì—°ì†ìœ¼ë¡œ ì‚¬ìš©ë˜ì—ˆìŠµë‹ˆë‹¤.",
                    "suggestion": "ë” ìì—°ìŠ¤ëŸ¬ìš´ í‘œí˜„ìœ¼ë¡œ ìˆ˜ì •í•´ë³´ì„¸ìš”."
                })
        
        return feedback

    def _analyze_specific_grammar(self, doc):
        """íŠ¹ì • ë¬¸ë²• ìš”ì†Œ ë¶„ì„"""
        feedback = []
        
        # ì‹œì œ ì¼ê´€ì„± ê²€ì‚¬
        tenses = []
        for token in doc:
            if token.pos_ == "VERB" and token.morph.get("Tense"):
                tenses.extend(token.morph.get("Tense"))
        
        if len(set(tenses)) > 1:
            feedback.append({
                "point": "ì‹œì œ ì¼ê´€ì„±",
                "details": "ë¬¸ì¥ ë‚´ì—ì„œ ì‹œì œê°€ ì¼ê´€ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.",
                "suggestion": "ê¸€ì˜ ë¬¸ë§¥ì— ë§ëŠ” ì¼ê´€ëœ ì‹œì œë¥¼ ì‚¬ìš©í•˜ì„¸ìš”."
            })
        
        return feedback

    def _find_positive_points(self, student_answer, model_answer):
        """ê¸ì •ì ì¸ ë¶€ë¶„ ì°¾ê¸°"""
        positive_points = []
        
        # ë¬¸ì¥ ê¸¸ì´ì˜ ì ì ˆì„± í‰ê°€
        student_sents = [sent.string.strip() for sent in self.nlp(student_answer).sents]
        if all(10 <= len(sent.split()) <= 25 for sent in student_sents):
            positive_points.append("ë¬¸ì¥ì˜ ê¸¸ì´ê°€ ì ì ˆí•˜ì—¬ ì½ê¸° ì‰½ìŠµë‹ˆë‹¤.")
        
        # ê³ ê¸‰ ì–´íœ˜ ì‚¬ìš© í‰ê°€
        student_doc = self.nlp(student_answer)
        advanced_words = [token.text for token in student_doc if token.is_alpha and len(token.text) > 8]
        if advanced_words:
            positive_points.append(f"'{', '.join(advanced_words[:3])}' ë“±ì˜ ê³ ê¸‰ ì–´íœ˜ë¥¼ ì ì ˆíˆ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤.")
        
        # ë¬¸ë²•ì  ì •í™•ì„± í‰ê°€
        grammar_errors = self._check_basic_grammar(self.nlp(student_answer))
        if len(grammar_errors) <= 2:
            positive_points.append("ì „ë°˜ì ìœ¼ë¡œ ë¬¸ë²•ì´ ì •í™•í•©ë‹ˆë‹¤.")
        
        # ëª¨ë²” ë‹µì•ˆê³¼ì˜ ìœ ì‚¬ë„ í‰ê°€
        student_doc = self.nlp(student_answer)
        model_doc = self.nlp(model_answer)
        similarity = student_doc.similarity(model_doc)
        
        if similarity > 0.8:
            positive_points.append("ëª¨ë²” ë‹µì•ˆì˜ í•µì‹¬ ë‚´ìš©ì„ ì˜ ë°˜ì˜í–ˆìŠµë‹ˆë‹¤.")
        elif similarity > 0.6:
            positive_points.append("ì£¼ìš” ë‚´ìš©ì„ ì ì ˆíˆ í¬í•¨í•˜ê³  ìˆìŠµë‹ˆë‹¤.")
        
        return positive_points

    def _calculate_overall_score(self, feedback):
        """ì¢…í•© ì ìˆ˜ ê³„ì‚°"""
        # ê¸°ë³¸ ì ìˆ˜ ì‹œì‘ (100ì  ë§Œì )
        score = 100
        
        # ë¬¸ë²• ì˜¤ë¥˜ë‹¹ ê°ì 
        grammar_deduction = len(feedback["grammar_feedback"]) * 5
        score -= min(grammar_deduction, 30)  # ìµœëŒ€ 30ì  ê°ì 
        
        # ì–´íœ˜ ì‚¬ìš© ë¬¸ì œë‹¹ ê°ì 
        vocabulary_deduction = len(feedback["vocabulary_feedback"]) * 5
        score -= min(vocabulary_deduction, 20)  # ìµœëŒ€ 20ì  ê°ì 
        
        # ë‚´ìš© ë¬¸ì œë‹¹ ê°ì 
        content_deduction = len(feedback["content_feedback"]) * 5
        score -= min(content_deduction, 30)  # ìµœëŒ€ 30ì  ê°ì 
        
        # ê¸ì •ì ì¸ ë¶€ë¶„ë‹¹ ê°€ì 
        positive_points = len(feedback["positive_points"]) * 2
        score += min(positive_points, 10)  # ìµœëŒ€ 10ì  ê°€ì 
        
        return max(0, min(score, 100))  # 0-100 ë²”ìœ„ ìœ ì§€

    def _generate_korean_summary(self, feedback):
        """í•œê¸€ ìš”ì•½ ìƒì„±"""
        summary = []
        
        # ì „ì²´ í‰ê°€ ìš”ì•½
        summary.append(f"ğŸ“Š ì¢…í•© ì ìˆ˜: {feedback['overall_score']}ì ")
        
        # ê¸ì •ì ì¸ ë¶€ë¶„
        if feedback["positive_points"]:
            summary.append("\nğŸ’ª ì˜í•œ ì :")
            for point in feedback["positive_points"]:
                summary.append(f"- {point}")
        
        # ë¬¸ë²• í”¼ë“œë°±
        if feedback["grammar_feedback"]:
            summary.append("\nğŸ“ ë¬¸ë²• ê´€ë ¨ ì˜ê²¬:")
            for error in feedback["grammar_feedback"][:3]:  # ì£¼ìš” ì˜¤ë¥˜ 3ê°œë§Œ
                summary.append(f"- {error['error']}")
                if 'suggestion' in error:
                    summary.append(f"  â†’ ì œì•ˆ: {error['suggestion']}")
        
        # ì–´íœ˜ í”¼ë“œë°±
        if feedback["vocabulary_feedback"]:
            summary.append("\nğŸ“š ì–´íœ˜ ê´€ë ¨ ì˜ê²¬:")
            for feedback_item in feedback["vocabulary_feedback"]:
                summary.append(f"- {feedback_item['point']}")
                if 'suggestion' in feedback_item:
                    summary.append(f"  â†’ ì œì•ˆ: {feedback_item['suggestion']}")
        
        # ë‚´ìš© í”¼ë“œë°±
        if feedback["content_feedback"]:
            summary.append("\nğŸ’¡ ë‚´ìš© ê´€ë ¨ ì˜ê²¬:")
            for feedback_item in feedback["content_feedback"]:
                summary.append(f"- {feedback_item['point']}")
                if 'suggestion' in feedback_item:
                    summary.append(f"  â†’ ì œì•ˆ: {feedback_item['suggestion']}")
        
        # ì „ì²´ì ì¸ ì œì•ˆ
        if feedback["suggestions"]:
            summary.append("\nâœ¨ í–¥ìƒì„ ìœ„í•œ ì œì•ˆ:")
            for suggestion in feedback["suggestions"]:
                summary.append(f"- {suggestion}")
        
        return "\n".join(summary) 